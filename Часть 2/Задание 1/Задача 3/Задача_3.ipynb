{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475dd96b",
   "metadata": {},
   "source": [
    "# Богданов Александр Иванович, Б05-003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c389666",
   "metadata": {},
   "source": [
    "## Модель автокодировщика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73bc170e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:03.063219Z",
     "start_time": "2024-04-13T04:35:03.057615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a46cc9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:03.542437Z",
     "start_time": "2024-04-13T04:35:03.531203Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3ac88",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22803a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:04.563373Z",
     "start_time": "2024-04-13T04:35:04.554804Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, word_to_ind, tokenizer):\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, sentences, max_length = 10, pad_to_max_length = False):\n",
    "        tokens = self.tokenizer.tokenize_sents(sentences)\n",
    "        if not pad_to_max_length:\n",
    "            max_length = min(max_length, max(map(len, tokens)))\n",
    "        tokens = [['[CLS]'] + s + ['[SEP]'] + ['[PAD]']*(max_length-len(s)) \\\n",
    "                  if len(s) < max_length \\\n",
    "                  else ['[CLS]'] + s[:max_length] + ['[SEP]'] \\\n",
    "                  for s in tokens ]\n",
    "        ids = [[self.word_to_ind.get(w, self.word_to_ind['[UNK]']) for w in sent] for sent in tokens]\n",
    "        return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e987c710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:30.557522Z",
     "start_time": "2024-04-13T04:35:30.554397Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_dict(dataset_train, trashhold=1):\n",
    "    helps = {}\n",
    "    for sent in tqdm(dataset_train.values[:, 1]):\n",
    "        for word in RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+').tokenize(sent):\n",
    "            if word in helps:\n",
    "                helps[word] += 1\n",
    "            else:\n",
    "                helps[word] = 1\n",
    "    \n",
    "    word2idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "    idx2word = {0: '[PAD]', 1: '[UNK]', 2: '[CLS]', 3: '[SEP]'}\n",
    "    for elem, number in helps.items():\n",
    "        if number >= trashhold and elem not in word2idx:\n",
    "            word2idx[elem] = len(word2idx)\n",
    "            idx2word[len(idx2word)] = elem\n",
    "    \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "744bdc2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:40.373825Z",
     "start_time": "2024-04-13T04:35:40.358209Z"
    }
   },
   "outputs": [],
   "source": [
    "def check(batch_size, dataset, model, loss_function, idx2word):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_generator = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "            \n",
    "    test_loss = 0\n",
    "    for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "        x_batch = x_batch.to(model.device)\n",
    "        y_batch = y_batch.to(model.device)\n",
    "                \n",
    "        output = model(x_batch)\n",
    "\n",
    "        test_loss += loss_function(output.transpose(1,2), y_batch).cpu().item()*len(x_batch)\n",
    "      \n",
    "    test_loss /= len(dataset)\n",
    "\n",
    "    print(f'loss: {test_loss}')\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    outputs = model(x)\n",
    "    \n",
    "    one_x = x[0].cpu().numpy()\n",
    "    one_output = outputs[0].argmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "    words = [idx2word[idx] for idx in one_x]\n",
    "    pred_words = [idx2word[idx] for idx in one_output]\n",
    "\n",
    "    table = PrettyTable([\"Word\", \"Predict\"])\n",
    "    table.align[\"Word\"], table.align[\"Predict\"] = \"l\", \"l\"\n",
    "\n",
    "    for word, pred in zip(words, pred_words):\n",
    "        if word != idx2word[word2idx['[PAD]']]:\n",
    "            table.add_row([word, pred])\n",
    "\n",
    "    print(table)\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c7c0c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:42.356440Z",
     "start_time": "2024-04-13T04:35:42.353885Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(x_batch.to(model.device))\n",
    "\n",
    "    loss = loss_function(output.transpose(1,2), y_batch.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2592bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:42.834989Z",
     "start_time": "2024-04-13T04:35:42.828402Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        \n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "            \n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c74f98fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:48.148348Z",
     "start_time": "2024-04-13T04:35:48.139488Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size, \n",
    "            dataset,\n",
    "            model, \n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "    \n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True), \n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size> 0))\n",
    "        \n",
    "        epoch_loss = train_epoch(train_generator=batch_generator, \n",
    "                    model=model, \n",
    "                    loss_function=loss_function, \n",
    "                    optimizer=optima, \n",
    "                    callback=callback)\n",
    "        \n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "febdc746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:35:53.989192Z",
     "start_time": "2024-04-13T04:35:53.982419Z"
    }
   },
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, delimeter = 300, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        model.eval()\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "        \n",
    "        if self.step % self.delimeter == 0:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            batch_generator = torch.utils.data.DataLoader(dataset=self.dataset, batch_size=self.batch_size)\n",
    "            \n",
    "            test_loss = 0\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "                y_batch = y_batch.to(model.device)\n",
    "\n",
    "                output = model(x_batch)\n",
    "\n",
    "                test_loss += self.loss_function(output.transpose(1,2), y_batch).cpu().item()*len(x_batch)\n",
    "            \n",
    "            test_loss /= len(self.dataset)\n",
    "\n",
    "            print(f'\\t\\tstep={self.step}, train_loss={loss}, val_loss={test_loss}')\n",
    "            \n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "          \n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de402f",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7f93a61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:38:21.306951Z",
     "start_time": "2024-04-13T05:38:21.302793Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, vocab_dim, emb_dim, latent_dim, num_layers=3, dropout=0, batch_norm=False):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        self.emb = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, latent_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        if batch_norm:\n",
    "            self.batch_norm = torch.nn.BatchNorm1d(latent_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.emb(x)\n",
    "        _, (h, c) = self.lstm(out)\n",
    "            \n",
    "        if self.batch_norm is not None:\n",
    "            out = self.batch_norm(out.transpose(1,2)).transpose(1,2)\n",
    "                \n",
    "        out = torch.cat([h, c], dim=-1).transpose(0, 1)[:, -1, :]\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9992f05b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:38:34.562176Z",
     "start_time": "2024-04-13T05:38:34.555743Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, vocab_dim, latent_dim, emb_dim, hidden_dim, num_layers=3, dropout=0, batch_norm=False):\n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.h0 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.c0 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        \n",
    "        self.emb = torch.nn.Embedding(1, emb_dim)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(emb_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_dim)\n",
    "\n",
    "    def forward(self, latent_vector):\n",
    "        \n",
    "        h = self.h0(latent_vector).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = self.c0(latent_vector).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        \n",
    "        emb = self.emb(torch.zeros(len(latent_vector), 1).long())\n",
    "        \n",
    "        logits = []\n",
    "        for i in range(12):\n",
    "            out, (h, c) = self.lstm(emb, (h, c))\n",
    "                \n",
    "            if self.batch_norm is not None:\n",
    "                out = self.batch_norm(out.transpose(1,2)).transpose(1,2)\n",
    "                \n",
    "            logits.append(out[:,-1,:])\n",
    "        \n",
    "        out = torch.stack(logits, 1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "48ea1a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:38:34.993715Z",
     "start_time": "2024-04-13T05:38:34.984184Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, vocab_dim, emb_dim, latent_dim, hidden_dim, num_layers=3, dropout=0, batch_norm=False):\n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_dim, emb_dim, latent_dim, num_layers, dropout, batch_norm)\n",
    "        self.decoder = Decoder(vocab_dim, 2 * latent_dim, emb_dim, hidden_dim, num_layers, dropout, batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911cea6",
   "metadata": {},
   "source": [
    "## Подключим tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a3cc19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T05:35:04.173491Z",
     "start_time": "2024-04-12T05:35:02.604847Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6aa2219f31b2367f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6aa2219f31b2367f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard_3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c804f1",
   "metadata": {},
   "source": [
    "## Скачаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "62543fdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:46:47.913200Z",
     "start_time": "2024-04-13T05:46:46.330983Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7c134e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:46:48.159414Z",
     "start_time": "2024-04-13T05:46:47.927935Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[['tag', 'message']].notnull().all(1)]\n",
    "dataset = dataset.sample(125000, random_state=42)\n",
    "train_mask = np.random.rand(len(dataset), ) < 0.8\n",
    "dataset_train = dataset[train_mask]\n",
    "dataset_test = dataset[~train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "64e919a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:37:37.245743Z",
     "start_time": "2024-04-13T05:37:37.052882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca5e7ead93b46f1a1328cb784babc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word2idx, idx2word = word_dict(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "37813a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:37:37.351165Z",
     "start_time": "2024-04-13T05:37:37.349216Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(word2idx, RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0463a168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:37:38.345582Z",
     "start_time": "2024-04-13T05:37:37.673146Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_sent = tokenizer(dataset_train.values[:, 1])\n",
    "test_data_sent = tokenizer(dataset_test.values[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fd79f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T05:37:38.352987Z",
     "start_time": "2024-04-13T05:37:38.349639Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train_pt = TensorDataset(train_data_sent, train_data_sent)\n",
    "dataset_test_pt = TensorDataset(test_data_sent, test_data_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a92fa",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e5b213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:26:11.422249Z",
     "start_time": "2024-04-13T04:26:11.415636Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d38139a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T04:26:11.937534Z",
     "start_time": "2024-04-13T04:26:11.932575Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_list = [10, 30, 50]\n",
    "num_layers_list = [3, 5, 7]\n",
    "dropout_list = [0, 0.3, 0.5]\n",
    "batch_norm_list = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "c555c594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T01:20:16.922753Z",
     "start_time": "2024-04-10T00:34:12.912809Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim = 10\n",
      "loss: 11.563268907107823, acc: 0.0\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | LOVATO  |\n",
      "| @       | LOVATO  |\n",
      "| Lottie  | LOVATO  |\n",
      "| does    | LOVATO  |\n",
      "| have    | LOVATO  |\n",
      "| but     | LOVATO  |\n",
      "| I       | LOVATO  |\n",
      "| Secrets | LOVATO  |\n",
      "| enough  | LOVATO  |\n",
      "| ,       | LOVATO  |\n",
      "| media   | LOVATO  |\n",
      "| [SEP]   | LOVATO  |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd3d23174954c2a800cd1f6f233af44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.299795627593994, val_loss=7.12644046951143, val_acc=0.06318147970852614\n",
      "\t\tstep=600, train_loss=6.401655197143555, val_loss=6.434572429316113, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.4732232093811035, val_loss=6.395022167968507, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.331718921661377, val_loss=6.372049239371521, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.281586170196533, val_loss=6.34330854717198, val_acc=0.08333333333333333\n",
      "loss: 6.333392953804203, acc: 0.16666666666666666\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [CLS]   |\n",
      "| @     | [SEP]   |\n",
      "| Your  | [SEP]   |\n",
      "| tird  | [SEP]   |\n",
      "| left  | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| from  | [SEP]   |\n",
      "| had   | [SEP]   |\n",
      "| beest | [SEP]   |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n",
      "dim = 30\n",
      "loss: 11.603923020607093, acc: 0.0\n",
      "+-----------+----------------+\n",
      "| Word      | Predict        |\n",
      "+-----------+----------------+\n",
      "| [CLS]     | MissGenevieveD |\n",
      "| Aden      | MissGenevieveD |\n",
      "| s         | nster          |\n",
      "| better    | nster          |\n",
      "| bg        | nster          |\n",
      "| afternoon | nster          |\n",
      "| hendy     | nster          |\n",
      "| bike      | nster          |\n",
      "| ride      | nster          |\n",
      "| short     | nster          |\n",
      "| short     | nster          |\n",
      "| [SEP]     | nster          |\n",
      "+-----------+----------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9655570ab94c0684cc04d1d78545bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.270175933837891, val_loss=6.423658425145615, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.227050304412842, val_loss=6.244577080586776, val_acc=0.1661879687250678\n",
      "\t\tstep=900, train_loss=6.067775726318359, val_loss=6.078313124242612, val_acc=0.16664007233657785\n",
      "\t\tstep=1200, train_loss=5.929986953735352, val_loss=5.932060687188538, val_acc=0.16730825488005957\n",
      "\t\tstep=1500, train_loss=5.965603351593018, val_loss=5.8387442323719085, val_acc=0.21379514387532578\n",
      "loss: 5.81597448007825, acc: 0.2166839529812244\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | [CLS]   |\n",
      "| contact  | @       |\n",
      "| Axwy     | @       |\n",
      "| HDQ      | @       |\n",
      "| Griffin  | -       |\n",
      "| 2        | -       |\n",
      "| morning  | -       |\n",
      "| spamming | -       |\n",
      "| exam     | [SEP]   |\n",
      "| tweeties | [SEP]   |\n",
      "| better   | [SEP]   |\n",
      "| [SEP]    | [SEP]   |\n",
      "+----------+---------+\n",
      "dim = 50\n",
      "loss: 11.618581306959085, acc: 3.3242912611031327e-06\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | jvgU    |\n",
      "| @        | tounge  |\n",
      "| Oop      | tounge  |\n",
      "| even     | tounge  |\n",
      "| -        | tounge  |\n",
      "| tat      | tounge  |\n",
      "| m        | tounge  |\n",
      "| spamming | tounge  |\n",
      "| already  | tounge  |\n",
      "| tellie   | tounge  |\n",
      "| snuggle  | tounge  |\n",
      "| [SEP]    | tounge  |\n",
      "+----------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd3b6b04d2149c49b3dd2a4ff69ef6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.355953693389893, val_loss=6.237808837224103, val_acc=0.1661879687250678\n",
      "\t\tstep=600, train_loss=5.834472179412842, val_loss=5.964174136716783, val_acc=0.1666866124142333\n",
      "\t\tstep=900, train_loss=5.958387851715088, val_loss=5.8269660668033, val_acc=0.2138150896228924\n",
      "\t\tstep=1200, train_loss=5.636808395385742, val_loss=5.769651941159743, val_acc=0.23065594915164087\n",
      "\t\tstep=1500, train_loss=5.630359649658203, val_loss=5.7268549810476195, val_acc=0.2333585979469177\n",
      "loss: 5.7233956420518615, acc: 0.23084543375352376\n",
      "+--------------+---------+\n",
      "| Word         | Predict |\n",
      "+--------------+---------+\n",
      "| [CLS]        | [CLS]   |\n",
      "| issues       | @       |\n",
      "| better       | [CLS]   |\n",
      "| cheerleaderr | I       |\n",
      "| plastered    | -       |\n",
      "| hurting      | -       |\n",
      "| SouthernBets | -       |\n",
      "| spamming     | -       |\n",
      "| and          | -       |\n",
      "| controll     | [PAD]   |\n",
      "| jaz          | [SEP]   |\n",
      "| [SEP]        | [SEP]   |\n",
      "+--------------+---------+\n"
     ]
    }
   ],
   "source": [
    "for dim in dim_list:\n",
    "    print(f'dim = {dim}')\n",
    "    \n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=dim, latent_dim=dim, hidden_dim=dim)\n",
    "    model.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'tensorboard_3/hidden_dim_{dim}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    \n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1, \n",
    "            batch_size=64, \n",
    "            dataset=dataset_train_pt,\n",
    "            model=model, \n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b9c09",
   "metadata": {},
   "source": [
    "Скорее всего очень простая модель для этой задачи, качество не очень. Но, чем больше размерность, тем лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "ac22de8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:06:04.449519Z",
     "start_time": "2024-04-10T01:20:53.632354Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "loss: 11.578179473870867, acc: 0.0\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | JMMcCoy |\n",
      "| @       | JMMcCoy |\n",
      "| Elena   | JMMcCoy |\n",
      "| Park    | JMMcCoy |\n",
      "| -       | JMMcCoy |\n",
      "| Feet    | JMMcCoy |\n",
      "| gallito | ewi     |\n",
      "| [SEP]   | ewi     |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c855eab074fe1ba05ebb0728f147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.106370449066162, val_loss=7.126648493849415, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.496007442474365, val_loss=6.4371417085767195, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.489595890045166, val_loss=6.398349907897735, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.308361530303955, val_loss=6.3605779610088, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.266372203826904, val_loss=6.327354115619602, val_acc=0.16666666666666666\n",
      "loss: 6.324021055231601, acc: 0.16666666666666666\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | [CLS]   |\n",
      "| @        | [SEP]   |\n",
      "| flyziks  | [SEP]   |\n",
      "| house    | [SEP]   |\n",
      "| halliday | [SEP]   |\n",
      "| working  | [SEP]   |\n",
      "| ICE      | [SEP]   |\n",
      "| pains    | [SEP]   |\n",
      "| -        | [SEP]   |\n",
      "| nieuwste | [SEP]   |\n",
      "| -        | [SEP]   |\n",
      "| [SEP]    | [SEP]   |\n",
      "+----------+---------+\n",
      "num_layers = 5\n",
      "loss: 11.620166636738306, acc: 0.0\n",
      "+-----------+---------------+\n",
      "| Word      | Predict       |\n",
      "+-----------+---------------+\n",
      "| [CLS]     | ColdHearted   |\n",
      "| @         | ColdHearted   |\n",
      "| wednesday | incaseyoucare |\n",
      "| drumming  | incaseyoucare |\n",
      "| dates     | incaseyoucare |\n",
      "| too       | incaseyoucare |\n",
      "| better    | incaseyoucare |\n",
      "| dropping  | incaseyoucare |\n",
      "| planet    | incaseyoucare |\n",
      "| markfinn  | incaseyoucare |\n",
      "| this      | incaseyoucare |\n",
      "| [SEP]     | incaseyoucare |\n",
      "+-----------+---------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9030b44206d845f6bf38fd472bbda4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.0511016845703125, val_loss=7.159190635200282, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.418607711791992, val_loss=6.449256312548496, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.348978519439697, val_loss=6.4002232479292065, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.201016902923584, val_loss=6.364319235091196, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.471507549285889, val_loss=6.331271079961089, val_acc=0.16666666666666666\n",
      "loss: 6.323718598418893, acc: 0.16666666666666666\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [CLS]   |\n",
      "| 2     | [SEP]   |\n",
      "| Some  | [SEP]   |\n",
      "| and   | [SEP]   |\n",
      "| Fell  | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| -     | [SEP]   |\n",
      "| Jeff  | [SEP]   |\n",
      "| hendy | [SEP]   |\n",
      "| hendy | [SEP]   |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n",
      "num_layers = 7\n",
      "loss: 11.579867020736799, acc: 0.0\n",
      "+--------------+------------+\n",
      "| Word         | Predict    |\n",
      "+--------------+------------+\n",
      "| [CLS]        | asheen     |\n",
      "| @            | alexfoster |\n",
      "| [UNK]        | alexfoster |\n",
      "| 9299         | alexfoster |\n",
      "| wonderful    | alexfoster |\n",
      "| gone         | alexfoster |\n",
      "| echothirteen | alexfoster |\n",
      "| like         | alexfoster |\n",
      "| on           | alexfoster |\n",
      "| unwired      | alexfoster |\n",
      "| Lisa         | alexfoster |\n",
      "| [SEP]        | alexfoster |\n",
      "+--------------+------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beab60f1d51c4a608ae585974a1765d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.214671611785889, val_loss=7.196025737986451, val_acc=0.014646827296420403\n",
      "\t\tstep=600, train_loss=6.351457595825195, val_loss=6.434693607432584, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.39912748336792, val_loss=6.3942550520244845, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.149362087249756, val_loss=6.363613786205866, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.497238636016846, val_loss=6.3277928188160235, val_acc=0.16666666666666666\n",
      "loss: 6.322121474623813, acc: 0.16666666666666666\n",
      "+--------------+---------+\n",
      "| Word         | Predict |\n",
      "+--------------+---------+\n",
      "| [CLS]        | [CLS]   |\n",
      "| @            | [SEP]   |\n",
      "| [UNK]        | [SEP]   |\n",
      "| Hoping       | [SEP]   |\n",
      "| unwired      | [SEP]   |\n",
      "| Gaga         | [SEP]   |\n",
      "| practically  | [SEP]   |\n",
      "| csellmybelle | [SEP]   |\n",
      "| wants        | [SEP]   |\n",
      "| [SEP]        | [SEP]   |\n",
      "+--------------+---------+\n"
     ]
    }
   ],
   "source": [
    "for num_layers in num_layers_list:\n",
    "    print(f'num_layers = {num_layers}')\n",
    "    \n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=128, latent_dim=128, hidden_dim=128, num_layers=num_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'tensorboard_3/num_layers_{num_layers}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    \n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1, \n",
    "            batch_size=64, \n",
    "            dataset=dataset_train_pt,\n",
    "            model=model, \n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addebbe7",
   "metadata": {},
   "source": [
    "При увеличении количества слоев, модель становится тяжелее и ей нужно будет больше эпох, чтобы обучиться. Но скорее всего она обучится до лучших результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8b2a50f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:52:32.326317Z",
     "start_time": "2024-04-10T02:07:20.004016Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout = 0\n",
      "loss: 11.63650400345265, acc: 0.0\n",
      "+-------------+---------------+\n",
      "| Word        | Predict       |\n",
      "+-------------+---------------+\n",
      "| [CLS]       | reedalexander |\n",
      "| @           | reedalexander |\n",
      "| pepsicans   | reedalexander |\n",
      "| promogeorge | reedalexander |\n",
      "| old         | newLeaks      |\n",
      "| had         | newLeaks      |\n",
      "| are         | newLeaks      |\n",
      "| media       | newLeaks      |\n",
      "| m           | newLeaks      |\n",
      "| [SEP]       | newLeaks      |\n",
      "+-------------+---------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67b4381c5934234a33932537d344e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.339432239532471, val_loss=7.270501530453864, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.399893283843994, val_loss=6.454840982551873, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.462089538574219, val_loss=6.40022532708226, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.333353519439697, val_loss=6.3793331764523105, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.421374797821045, val_loss=6.342741963204079, val_acc=0.08333333333333333\n",
      "loss: 6.3379026471246505, acc: 0.08333333333333333\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | [SEP]   |\n",
      "| @        | [SEP]   |\n",
      "| Sdh      | [SEP]   |\n",
      "| hiks     | [SEP]   |\n",
      "| hendy    | [SEP]   |\n",
      "| HOMETOWN | [SEP]   |\n",
      "| hopefull | [SEP]   |\n",
      "| use      | [SEP]   |\n",
      "| on       | [SEP]   |\n",
      "| Duty     | [SEP]   |\n",
      "| hendy    | [SEP]   |\n",
      "| [SEP]    | [SEP]   |\n",
      "+----------+---------+\n",
      "dropout = 0.3\n",
      "loss: 11.625426612593483, acc: 0.0\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | puckish |\n",
      "| @         | Epitaph |\n",
      "| suzuki    | gullens |\n",
      "| I         | gullens |\n",
      "| pooch     | gullens |\n",
      "| Wednesday | gullens |\n",
      "| and       | gullens |\n",
      "| doc       | gullens |\n",
      "| jazzjeet  | gullens |\n",
      "| -         | gullens |\n",
      "| cause     | gullens |\n",
      "| [SEP]     | gullens |\n",
      "+-----------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c22d5bd552b405fbd37e7b5e57153ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.194221496582031, val_loss=7.211782546216114, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.362119197845459, val_loss=6.4310295197197895, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.431772232055664, val_loss=6.388828406859685, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.312370777130127, val_loss=6.354995869973905, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.258993625640869, val_loss=6.32839735018041, val_acc=0.16666666666666666\n",
      "loss: 6.3249012066147365, acc: 0.16666666666666666\n",
      "+--------+---------+\n",
      "| Word   | Predict |\n",
      "+--------+---------+\n",
      "| [CLS]  | [CLS]   |\n",
      "| @      | [SEP]   |\n",
      "| awl    | [SEP]   |\n",
      "| louise | [SEP]   |\n",
      "| final  | [SEP]   |\n",
      "| f      | [SEP]   |\n",
      "| had    | [SEP]   |\n",
      "| are    | [SEP]   |\n",
      "| keep   | [SEP]   |\n",
      "| m      | [SEP]   |\n",
      "| I      | [SEP]   |\n",
      "| [SEP]  | [SEP]   |\n",
      "+--------+---------+\n",
      "dropout = 0.5\n",
      "loss: 11.638147595473905, acc: 0.0\n",
      "+----------+------------+\n",
      "| Word     | Predict    |\n",
      "+----------+------------+\n",
      "| [CLS]    | whysogreen |\n",
      "| @        | whysogreen |\n",
      "| 922      | whysogreen |\n",
      "| box      | whysogreen |\n",
      "| with     | whysogreen |\n",
      "| spamming | whysogreen |\n",
      "| day      | whysogreen |\n",
      "| s        | whysogreen |\n",
      "| and      | whysogreen |\n",
      "| brulee   | whysogreen |\n",
      "| wants    | whysogreen |\n",
      "| [SEP]    | whysogreen |\n",
      "+----------+------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fab3af22bc43ff94f0272151cdc75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.387423992156982, val_loss=7.238711191627499, val_acc=0.03973857773522685\n",
      "\t\tstep=600, train_loss=6.224098205566406, val_loss=6.445171303168541, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.300529956817627, val_loss=6.395610649239147, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.27586030960083, val_loss=6.371978462950282, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.527946949005127, val_loss=6.3252047692757385, val_acc=0.16666666666666666\n",
      "loss: 6.322092158687501, acc: 0.16666666666666666\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| @       | [SEP]   |\n",
      "| [UNK]   | [SEP]   |\n",
      "| Secrets | [SEP]   |\n",
      "| what    | [SEP]   |\n",
      "| -       | [SEP]   |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "for dropout in dropout_list:\n",
    "    print(f'dropout = {dropout}')\n",
    "    \n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=128, latent_dim=128, hidden_dim=128, dropout=dropout)\n",
    "    model.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'tensorboard_3/dropout_{dropout}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    \n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1, \n",
    "            batch_size=64, \n",
    "            dataset=dataset_train_pt,\n",
    "            model=model, \n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a94d56",
   "metadata": {},
   "source": [
    "Как и ожидалось, Dropout не улучшает качество обучения модели - он нужен для регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "7b67c256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:25:20.915511Z",
     "start_time": "2024-04-10T02:55:14.930820Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_norm = False\n",
      "loss: 11.592061482416264, acc: 0.0\n",
      "+------------+----------+\n",
      "| Word       | Predict  |\n",
      "+------------+----------+\n",
      "| [CLS]      | hostages |\n",
      "| sell       | hostages |\n",
      "| better     | jsyk     |\n",
      "| vegetarian | juror    |\n",
      "| s          | juror    |\n",
      "| better     | juror    |\n",
      "| Paso       | juror    |\n",
      "| hurting    | juror    |\n",
      "| bff        | juror    |\n",
      "| put        | juror    |\n",
      "| -          | juror    |\n",
      "| [SEP]      | juror    |\n",
      "+------------+----------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5e657c1da94c4b9fb7e7bb07fc8315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.217648029327393, val_loss=7.180011480452247, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.3778395652771, val_loss=6.430727429950715, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.323089599609375, val_loss=6.384221692432984, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.261751651763916, val_loss=6.362088769213024, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=6.411561489105225, val_loss=6.334267057184779, val_acc=0.16666666666666666\n",
      "loss: 6.329276874530671, acc: 0.16666666666666666\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| @       | [SEP]   |\n",
      "| CE      | [SEP]   |\n",
      "| gone    | [SEP]   |\n",
      "| nap     | [SEP]   |\n",
      "| 2       | [SEP]   |\n",
      "| life    | [SEP]   |\n",
      "| journal | [SEP]   |\n",
      "| I       | [SEP]   |\n",
      "| WE      | [SEP]   |\n",
      "| ,       | [SEP]   |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n",
      "batch_norm = True\n",
      "loss: 11.585652138031477, acc: 0.0\n",
      "+------------+-------------+\n",
      "| Word       | Predict     |\n",
      "+------------+-------------+\n",
      "| [CLS]      | cornywallis |\n",
      "| I          | cornywallis |\n",
      "| sot        | cornywallis |\n",
      "| studying   | sall        |\n",
      "| ALWAYS     | sall        |\n",
      "| bites      | sall        |\n",
      "| moving     | sall        |\n",
      "| definitely | sall        |\n",
      "| the        | sall        |\n",
      "| 2011       | sall        |\n",
      "| m          | sall        |\n",
      "| [SEP]      | sall        |\n",
      "+------------+-------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd21d46978da4d0f9f4fd10f862993b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.847625255584717, val_loss=7.751521928850232, val_acc=0.27943659911706825\n",
      "\t\tstep=600, train_loss=5.582111835479736, val_loss=5.552311638125548, val_acc=0.30870365937982025\n",
      "\t\tstep=900, train_loss=5.070997714996338, val_loss=5.287894881549627, val_acc=0.3286228126163502\n",
      "\t\tstep=1200, train_loss=5.294387340545654, val_loss=5.210344064857698, val_acc=0.3330740386149673\n",
      "\t\tstep=1500, train_loss=4.986623764038086, val_loss=5.155622458028056, val_acc=0.3425515930003723\n",
      "loss: 5.116979391738067, acc: 0.3371296739535131\n",
      "+------------+---------+\n",
      "| Word       | Predict |\n",
      "+------------+---------+\n",
      "| [CLS]      | [CLS]   |\n",
      "| I          | @       |\n",
      "| random     | I       |\n",
      "| jessv      | ,       |\n",
      "| studying   | -       |\n",
      "| Elliptical | -       |\n",
      "| german     | -       |\n",
      "| this       | ,       |\n",
      "| will       | ,       |\n",
      "| ran        | ,       |\n",
      "| on         | wants   |\n",
      "| [SEP]      | [SEP]   |\n",
      "+------------+---------+\n"
     ]
    }
   ],
   "source": [
    "for batch_norm in batch_norm_list:\n",
    "    print(f'batch_norm = {batch_norm}')\n",
    "    \n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=128, latent_dim=128, hidden_dim=128, batch_norm=batch_norm)\n",
    "    model.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'tensorboard_3/batch_norm_{batch_norm}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    \n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1, \n",
    "            batch_size=64, \n",
    "            dataset=dataset_train_pt,\n",
    "            model=model, \n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a444daa",
   "metadata": {},
   "source": [
    "Добавление BatchNorm сильно улучшило качество модели, что ожидаемо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "c882a994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T04:31:12.853467Z",
     "start_time": "2024-04-10T04:08:40.492963Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trashhold = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9969c1d30a24e1da512e3b9eccae07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.641769719005906, acc: 0.0\n",
      "+------------+-----------+\n",
      "| Word       | Predict   |\n",
      "+------------+-----------+\n",
      "| [CLS]      | Dominos   |\n",
      "| @          | Dominos   |\n",
      "| ailynonyou | Dominos   |\n",
      "| i          | Dominos   |\n",
      "| cant       | hotpocket |\n",
      "| sleep      | hotpocket |\n",
      "| !          | hotpocket |\n",
      "| keep       | pined     |\n",
      "| me         | pined     |\n",
      "| company    | pined     |\n",
      "| [UNK]      | pined     |\n",
      "| [SEP]      | pined     |\n",
      "+------------+-----------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865fd9eb874749028d8533b98c927b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.452328205108643, val_loss=7.32226697097883, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=6.661736965179443, val_loss=6.5412748315335465, val_acc=0.0\n",
      "\t\tstep=900, train_loss=6.441305160522461, val_loss=6.495719717548322, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=6.388115406036377, val_loss=6.457355269941394, val_acc=0.16666666666666666\n",
      "\t\tstep=1500, train_loss=6.325118541717529, val_loss=6.42464259193144, val_acc=0.16666666666666666\n",
      "loss: 6.419886765343846, acc: 0.16666666666666666\n",
      "+--------+---------+\n",
      "| Word   | Predict |\n",
      "+--------+---------+\n",
      "| [CLS]  | [CLS]   |\n",
      "| I      | [SEP]   |\n",
      "| '      | [SEP]   |\n",
      "| m      | [SEP]   |\n",
      "| soooo  | [SEP]   |\n",
      "| bored  | [SEP]   |\n",
      "| .      | [SEP]   |\n",
      "| just   | [SEP]   |\n",
      "| fuckin | [SEP]   |\n",
      "| got    | [SEP]   |\n",
      "| a      | [SEP]   |\n",
      "| [SEP]  | [SEP]   |\n",
      "+--------+---------+\n",
      "trashhold = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff12cf57c4d4c818ab6eb75da1da016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.418727183733829, acc: 2.327003882772193e-05\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | Danny   |\n",
      "| Lol     | Danny   |\n",
      "| no      | Danny   |\n",
      "| problem | Danny   |\n",
      "| !       | Danny   |\n",
      "| K       | Danny   |\n",
      "| talk    | Danny   |\n",
      "| 2       | Danny   |\n",
      "| u       | Danny   |\n",
      "| manana  | Danny   |\n",
      "| !       | Danny   |\n",
      "| [SEP]   | Danny   |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777451a1bd11463bbbbcce3bf514a0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.464632034301758, val_loss=6.348317198649117, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=5.85308837890625, val_loss=5.821488175377694, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=6.065928936004639, val_loss=5.778623581769574, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=5.964488983154297, val_loss=5.755351783480963, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=5.809302806854248, val_loss=5.728145563071741, val_acc=0.16666666666666666\n",
      "loss: 5.723182030646296, acc: 0.16666666666666666\n",
      "+-------------+---------+\n",
      "| Word        | Predict |\n",
      "+-------------+---------+\n",
      "| [CLS]       | [CLS]   |\n",
      "| @           | [SEP]   |\n",
      "| clarasdiary | [SEP]   |\n",
      "| i           | [SEP]   |\n",
      "| live        | [SEP]   |\n",
      "| in          | [SEP]   |\n",
      "| #           | [SEP]   |\n",
      "| [UNK]       | [SEP]   |\n",
      "| i           | [SEP]   |\n",
      "| know        | [SEP]   |\n",
      "| .           | [SEP]   |\n",
      "| [SEP]       | [SEP]   |\n",
      "+-------------+---------+\n",
      "trashhold = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7618f438854d319a5a723cff5b54cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.000449587211895, acc: 0.0\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | Sue     |\n",
      "| ND    | Sue     |\n",
      "| TO    | kaffy   |\n",
      "| GET   | kaffy   |\n",
      "| READY | kaffy   |\n",
      "| FOR   | kaffy   |\n",
      "| WORK  | kaffy   |\n",
      "| [SEP] | kaffy   |\n",
      "+-------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1588abf79ad440f89730f85e1e5999e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.173194408416748, val_loss=6.167616246818298, val_acc=0.08333333333333333\n",
      "\t\tstep=600, train_loss=5.7625203132629395, val_loss=5.714588714935609, val_acc=0.08333333333333333\n",
      "\t\tstep=900, train_loss=5.7790608406066895, val_loss=5.67896885177213, val_acc=0.08333333333333333\n",
      "\t\tstep=1200, train_loss=5.850861072540283, val_loss=5.662266826979639, val_acc=0.08333333333333333\n",
      "\t\tstep=1500, train_loss=5.795910358428955, val_loss=5.639554495329832, val_acc=0.08333333333333333\n",
      "loss: 5.6348826999734305, acc: 0.08333333333333333\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [SEP]   |\n",
      "| @     | [SEP]   |\n",
      "| [UNK] | [SEP]   |\n",
      "| [UNK] | [SEP]   |\n",
      "| what  | [SEP]   |\n",
      "| up    | [SEP]   |\n",
      "| I     | [SEP]   |\n",
      "| leave | [SEP]   |\n",
      "| July  | [SEP]   |\n",
      "| 7     | [SEP]   |\n",
      "| th    | [SEP]   |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n"
     ]
    }
   ],
   "source": [
    "for trashhold in [1, 2, 3]:\n",
    "    print(f'trashhold = {trashhold}')\n",
    "    \n",
    "    word2idx, idx2word = word_dict(dataset_train, trashhold=trashhold)\n",
    "    tokenizer = Tokenizer(word2idx, RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+'))\n",
    "    train_data_sent = tokenizer(dataset_train.values[:, 1])\n",
    "    test_data_sent = tokenizer(dataset_test.values[:, 1])\n",
    "    dataset_train_pt = TensorDataset(train_data_sent, train_data_sent)\n",
    "    dataset_test_pt = TensorDataset(test_data_sent, test_data_sent)\n",
    "    \n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=128, latent_dim=128, hidden_dim=128,)\n",
    "    model.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'tensorboard_3/trashhold_{trashhold}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    \n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1, \n",
    "            batch_size=64, \n",
    "            dataset=dataset_train_pt,\n",
    "            model=model, \n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6253e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T01:20:17.032194Z",
     "start_time": "2024-04-10T01:20:17.032189Z"
    }
   },
   "source": [
    "При уменьшении размера словаря модель лучше улавливает ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f82664",
   "metadata": {},
   "source": [
    "Были проведены эксперименты над автоинкодером с подбором параметров."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
